---
title: "Estimate nu via Kurtosis"
output: html_document
author: ZHOU Rui
date: "`r Sys.Date()`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

This file is a trial on estimating the degrees of freedom $\nu$ via the Kurtosis. Here Kurtosis is a a measure of the "tailedness" of the probability distribution of a real-valued random variable, see also [wiki](https://en.wikipedia.org/wiki/Kurtosis). Basically, the expression of computing sample Kurtosis is: $$\text{Kurtosis}\left[X\right]=\text{E}\left[\left(\frac{X-\mu}{\sigma}\right)^{4}\right]=\frac{\text{E}\left[\left(X-\mu\right)^{4}\right]}{\left(\text{E}\left[\left(X-\mu\right)^{2}\right]\right)^{2}},$$ where $\mu$ and $\sigma$ are the sample mean and  standard deviation. Opposite to the degrees of freedom, smaller Kurtosis value means a heavier tail. The kurtosis of a normal distribution equals 3. An excess kurtosis is a metric that compares the kurtosis of a distribution against the kurtosis of a normal distribution, i.e., $$\hat{k} = \text{Kurtosis}\left[X\right] - 3.$$ 
To correct the bias of the sample Kurtosis, an unbiased estimator is commonly used $$\hat{\text{kurt}}(X) = \frac{T-1}{(T-2)(T-3)}\left((T+1)\hat{k} + 6\right).$$
For a multivariate distribution, in which we index each variable by $i$ ($1\le i \le N$), we define $\hat{\kappa}$ as $$\hat{\kappa} = \max{\left(-\frac{2}{N+2}, \frac{1}{3}\frac{1}{N}\sum_{i=1}^{N}\hat{\text{kurt}}(X_i)\right)}.$$ In the case of a Student $t$ distribution, one can simply use $\hat{\kappa} = \frac{2}{\hat{\nu} - 4}$, which says $$\hat{\nu} = \frac{2}{\hat{\kappa}} + 4$$

# Numerical Simulation

First genereate the multivariate Student $t$ data. Let us make it a function for late usage
```{r}
genX <- function(N = 10, T = round(N*1.1), nu = 5, mu = rep(0, N)) {
  U <- t(mvtnorm::rmvnorm(n = round(1.1*N), sigma = 0.1*diag(N)))
  Sigma <- U %*% t(U) + diag(N)
  Sigma_scale <- (nu-2)/nu * Sigma
  X <- mvtnorm::rmvt(n = T, delta = mu, sigma = Sigma_scale, df = nu)
  return(X)
}
```

Then define functions for estimate $\nu$ via kurtosis
```{r}
excess_kurtosis_unbias <- function(x) {
  x <- as.vector(x)
  T <- length(x)
  excess_kurt <- PerformanceAnalytics::kurtosis(x, method = "excess") # mean(x_demean^4) / (mean(x_demean^2))^2 - 3
  excess_kurt_unbias <- excess_kurt * (T-1) / (T-2) / (T-3) * ((T+1)*excess_kurt + 6)
  return(excess_kurt_unbias)
}

est_nu_kurtosis <- function(X, comb_fun = mean) {
  kurt <- apply(X, 2, excess_kurtosis_unbias)
  kappa <- max(-2/(ncol(X)+2), comb_fun(kurt)/3)
  nu <- 2 / kappa + 4
  # in case of some funny results
  if (nu < 2) nu <- 2
  if (nu > 100) nu <- 100
  
  return(nu)
}
```

### Compare estimation performance with $T$ changes

```{r}
repeate <- 1e3
N <- 10
T_pool <- 10:20
res <- list()
for (i in 1:length(T_pool)) {
  T <- T_pool[i]
  # print(T)
  res[[i]] <- sapply(1:repeate, function(idx) {est_nu_kurtosis(genX(N = N, T = T))})
}

sapply(res, median)

library(ggplot2)
data <- reshape2::melt(data.frame("T" = T_pool, do.call(rbind, res)), id.vars = "T")
data$T <- as.factor(data$T)
ggplot(data, aes(x = T, y = value)) + geom_boxplot()
# ggplot(data, aes(x = T, y = value)) + geom_violin()
```
It seems that using the kurtosis to estimate $\nu$ gives can probably provide us a meansing reference. However, we notice $\hat{\nu}$ still has some outliers. So we should use it as a regularization target, and the weight should be decreased with $T/N$ increases.  

### Compare estimation performance with $\nu$ changes

```{r}
N <- 10
T <- 15
nu_pool <- 3:20
res <- list()
for (i in 1:length(nu_pool)) {
  nu <- nu_pool[i]
  # print(nu)
  res[[i]] <- sapply(1:repeate, function(idx) {est_nu_kurtosis(genX(N = N, T = T, nu = nu))})
}

sapply(res, median)

library(ggplot2)
data <- reshape2::melt(data.frame("nu" = nu_pool, do.call(rbind, res)), id.vars = "nu")
data$nu <- as.factor(data$nu)
ggplot(data, aes(x = nu, y = value)) + geom_boxplot()
# ggplot(data, aes(x = nu, y = value)) + geom_violin()
```


# Replace the mean of $\hat{\text{kurt}}$ by median value

### Compare estimation performance with $T$ changes

```{r}
N <- 10
res <- list()
for (i in 1:length(T_pool)) {
  T <- T_pool[i]
  # print(T)
  res[[i]] <- sapply(1:repeate, function(idx) {est_nu_kurtosis(genX(N = N, T = T), comb_fun = median)})
}

sapply(res, median)

library(ggplot2)
data <- reshape2::melt(data.frame("T" = T_pool, do.call(rbind, res)), id.vars = "T")
data$T <- as.factor(data$T)
ggplot(data, aes(x = T, y = value)) + geom_boxplot()
# ggplot(data, aes(x = T, y = value)) + geom_violin()
```

### Compare estimation performance with $\nu$ changes

```{r}
N <- 10
T <- 15
res <- list()
for (i in 1:length(nu_pool)) {
  nu <- nu_pool[i]
  # print(nu)
  res[[i]] <- sapply(1:repeate, function(idx) {est_nu_kurtosis(genX(N = N, T = T, nu = nu), comb_fun = median)})
}

sapply(res, median)

library(ggplot2)
data <- reshape2::melt(data.frame("nu" = nu_pool, do.call(rbind, res)), id.vars = "nu")
data$nu <- as.factor(data$nu)
ggplot(data, aes(x = nu, y = value)) + geom_boxplot()
# ggplot(data, aes(x = nu, y = value)) + geom_violin()
```

Well, it seems that using median value is worse than using the simple mean value.


# Try with large $N$

```{r}
N <- 100
T_pool <- seq(10, 200, 10)
res <- list()
for (i in 1:length(T_pool)) {
  T <- T_pool[i]
  # print(T)
  res[[i]] <- sapply(1:repeate, function(idx) {est_nu_kurtosis(genX(N = N, T = T))})
}

sapply(res, median)

library(ggplot2)
data <- reshape2::melt(data.frame("T" = T_pool, do.call(rbind, res)), id.vars = "T")
data$T <- as.factor(data$T)
ggplot(data, aes(x = T, y = value)) + geom_boxplot()
# ggplot(data, aes(x = T, y = value)) + geom_violin()
```

It seems with large $N$, the etimators via kurtosis becomes really good.

# Compare estimation performance with $N$ changes

Now we fix the ratio $T/N$ be $1.1$ and change $N$ from $10$ to $100$ to see the results

```{r}
N_pool <- seq(10, 100, 10)
res <- list()
for (i in 1:length(N_pool)) {
  N <- N_pool[i]
  # print(T)
  res[[i]] <- sapply(1:repeate, function(idx) {est_nu_kurtosis(genX(N = N, T = round(N*1.1)))})
}

sapply(res, median)

library(ggplot2)
data <- reshape2::melt(data.frame("N" = N_pool, do.call(rbind, res)), id.vars = "N")
data$N <- as.factor(data$N)
ggplot(data, aes(x = N, y = value)) + geom_boxplot()
# ggplot(data, aes(x = T, y = value)) + geom_violin()
```

It is clear that when $N$ increase, the estimation performance also improves. An explanation for that could be: the simple average of $\hat{\text{kurt}}$ become more reliable when $N$ goes large.
