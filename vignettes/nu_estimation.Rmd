---
title: "Estimation of nu"
author: |
  | Daniel P. Palomar
  | The Hong Kong University of Science and Technology (HKUST)
date: "`r Sys.Date()`"
output:
  prettydoc::html_pretty:
    theme: tactile
    highlight: vignette
    toc: no
    toc_depth: 2
  html_document:
    theme: flatly
    highlight: pygments  
    toc: yes
    toc_depth: 2
params:
  N_realiz: 10
csl: ieee.csl
bibliography: refs.bib
---

```{r, echo = FALSE}
library(knitr)
opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.align = "center",
  fig.retina = 2,
  out.width = "85%",
  dpi = 96,
  pngquant = "--speed=1"
)
knitr_in_progress <- isTRUE(getOption('knitr.in.progress'))
knit_hooks$set(pngquant = hook_pngquant)
# rmarkdown::render("vignettes/nu_estimation.Rmd", "html_document", params = list(N_realiz = 500))
```

-----------

> This report explores the convergence of $\nu$ for different estimation methods in package [`fitHeavyTail`](https://CRAN.R-project.org/package=fitHeavyTail).


Let's define the parameters for the generation of the multivariate Student's $t$ distribution:
```{r}
library(mvtnorm)  # package for multivariate t distribution
set.seed(137)

N <- 10   # number of variables
T <- 60   # number of observations
nu <- 8   # degrees of freedom for tail heavyness
mu <- rep(0, N)
U <- t(rmvnorm(n = round(0.3*N), sigma = 0.1*diag(N)))
Sigma_cov <- U %*% t(U) + diag(N)  # covariance matrix with factor model structure
Sigma_scatter <- (nu-2)/nu * Sigma_cov
```


To warm up, let's consider the following estimators for $\nu$:

- based on the kurtosis directly
- based on MLE w.r.t. $\nu$ assuming a diagonal sample covariance matrix
- idem but resampling for additional stability
- iterative estimation based on the EM algorithm with the previous three initial points. 

```{r}
library(fitHeavyTail)

X <- rmvt(n = T, delta = mu, sigma = Sigma_scatter, df = nu)  # generate data

# direct methods
nu_kurtosis           <- fit_mvt(X, nu = "kurtosis")$nu
nu_mle_diag           <- fit_mvt(X, nu = "MLE-diag")$nu
nu_mle_diag_resampled <- fit_mvt(X, nu = "MLE-diag-resampled")$nu

# EM with different initial points
EM_nu0_true               <- fit_mvt(X, initial = list(nu = nu))$nu
EM_nu0_kurtosis           <- fit_mvt(X, initial = list(nu = "kurtosis"))$nu
EM_nu0_mle_diag           <- fit_mvt(X, initial = list(nu = "MLE-diag"))$nu
EM_nu0_mle_diag_resampled <- fit_mvt(X, initial = list(nu = "MLE-diag-resampled"))$nu

rbind("nu_true" = nu, nu_kurtosis, nu_mle_diag, nu_mle_diag_resampled,
      EM_nu0_true, EM_nu0_kurtosis, EM_nu0_mle_diag, EM_nu0_mle_diag_resampled)
```

We can observe the convergence of $\nu$:
```{r}
fitted <- fit_mvt(X, return_iterates = TRUE)
fitHeavyTail:::plot_convergence(fitted)$nu
```


We now run `r params$N_realiz` Monte Carlo simulations.
```{r, echo=FALSE}
MSE <- function(Sigma_hat) norm(Sigma_hat - Sigma_cov, "F")^2

eval_single <- function(X) {
  nu <- MSE_cov <- num_iterations <- cpu_time <- list()
  
  # name                  <- "nu = 6"
  # fitted                <- fit_mvt(X, nu = 6)
  # nu[name]              <- fitted$nu
  # MSE_cov[name]         <- MSE(fitted$cov)
  # num_iterations[name]  <- fitted$num_iterations
  # cpu_time[name]        <- fitted$cpu_time

  name                  <- "nu = kurtosis"
  fitted                <- fit_mvt(X, nu = "kurtosis")
  nu[name]              <- fitted$nu
  MSE_cov[name]         <- MSE(fitted$cov)
  num_iterations[name]  <- fitted$num_iterations
  cpu_time[name]        <- fitted$cpu_time
  
  name                  <- "nu = MLE-diag"
  fitted                <- fit_mvt(X, nu = "MLE-diag")
  nu[name]              <- fitted$nu
  MSE_cov[name]         <- MSE(fitted$cov)
  num_iterations[name]  <- fitted$num_iterations
  cpu_time[name]        <- fitted$cpu_time

  name                  <- "nu = MLE-diag-resampled"
  fitted                <- fit_mvt(X, nu = "MLE-diag-resampled")
  nu[name]              <- fitted$nu
  MSE_cov[name]         <- MSE(fitted$cov)
  num_iterations[name]  <- fitted$num_iterations
  cpu_time[name]        <- fitted$cpu_time
  
  # name                  <- "nu0 = 6"
  # fitted                <- fit_mvt(X, initial = list(nu = 6), method_nu = "ECME-diag")
  # nu[name]              <- fitted$nu
  # MSE_cov[name]         <- MSE(fitted$cov)
  # num_iterations[name]  <- fitted$num_iterations
  # cpu_time[name]        <- fitted$cpu_time

  name                  <- "nu0 = kurtosis"
  fitted                <- fit_mvt(X, initial = list(nu = "kurtosis"))
  nu[name]              <- fitted$nu
  MSE_cov[name]         <- MSE(fitted$cov)
  num_iterations[name]  <- fitted$num_iterations
  cpu_time[name]        <- fitted$cpu_time

  name                  <- "nu0 = MLE-diag"
  fitted                <- fit_mvt(X, initial = list(nu = "MLE-diag"))
  nu[name]              <- fitted$nu
  MSE_cov[name]         <- MSE(fitted$cov)
  num_iterations[name]  <- fitted$num_iterations
  cpu_time[name]        <- fitted$cpu_time
  
  name                  <- "nu0 = MLE-diag-resampled"
  fitted                <- fit_mvt(X, initial = list(nu = "MLE-diag-resampled"))
  nu[name]              <- fitted$nu
  MSE_cov[name]         <- MSE(fitted$cov)
  num_iterations[name]  <- fitted$num_iterations
  cpu_time[name]        <- fitted$cpu_time

  return(list("nu"= nu, "MSE_cov" = MSE_cov, "num_iterations" = num_iterations, "cpu_time" = cpu_time))
}

T_sweep <- round(seq(from = 20, to = 100, by = 10))
if (!knitr_in_progress) pbar <- txtProgressBar(min = it <- 0, max = length(T_sweep), style = 3)
res_all_T <- list()
for(T in T_sweep) {
  if (!knitr_in_progress) setTxtProgressBar(pbar, it <- it + 1)
  
  # first, generate random heavy-tailed data sequentially for reproducibility
  X_list <- replicate(params$N_realiz, rmvt(n = T, delta = mu, sigma = Sigma_scatter, df = nu), simplify = FALSE)
  names(X_list) <- paste0("realiz ", 1:params$N_realiz)

  # then, run estimations for all realizations
  res_all_T <- c(res_all_T, list(lapply(X_list, eval_single)))
}
names(res_all_T) <- T_sweep
```
The results can be seen in the following boxplots:
```{r}
library(reshape2)

# create data.frame by melting the nested list
res_all_T_molten <- melt(res_all_T)
names(res_all_T_molten) <- c("value", "method", "measure", "realization", "T")
res_all_T_molten$realization <- NULL  # don't really need the realization index
res_all_T_molten$T      <- factor(res_all_T_molten$T, levels = T_sweep)
res_all_T_molten$method <- factor(res_all_T_molten$method, levels = names(res_all_T[[1]][[1]][[1]]))
```

```{r, echo=FALSE, fig.width = 9, fig.height = 5, out.width = "100%"}
library(ggplot2)
ggplot(res_all_T_molten[res_all_T_molten$measure == "nu", ], aes(x = T, y = value, fill = method)) +
  geom_boxplot(show.legend = FALSE) +  #show.legend = FALSE
  facet_wrap(~ method, dir = "v", scales = "free_y") +
  #coord_cartesian(ylim = c(0, 25)) +
  geom_hline(yintercept = nu, linetype = "dashed") +
  labs(title = sprintf("Estimated nu (nu = %d, N = %d)", nu, N), x = "T", y = "nu")
```

```{r, echo=FALSE, fig.width = 9, fig.height = 5, out.width = "100%"}
ggplot(res_all_T_molten[res_all_T_molten$measure == "MSE_cov", ], aes(x = T, y = value, fill = method)) +
  geom_boxplot(show.legend = FALSE) +  #show.legend = FALSE
  facet_wrap(~ method, dir = "v", scales = "free_y") +
  #coord_cartesian(ylim = c(0, 25)) +
  labs(title = sprintf("Estimated error in covariance matrix (nu = %d, N = %d)", nu, N), x = "T", y = "error")
```

```{r, echo=FALSE, fig.width = 9, fig.height = 5, out.width = "100%"}
ggplot(res_all_T_molten[res_all_T_molten$measure == "num_iterations", ], aes(x = T, y = value, fill = method)) +
  geom_boxplot(show.legend = FALSE) +
  facet_wrap(~ method, dir = "v", scales = "free_y") +
  labs(title = "Iterations for the estimation of all parameters", x = "T", y = "iterations")
```

```{r, echo=FALSE, fig.width = 9, fig.height = 5, out.width = "100%"}
ggplot(res_all_T_molten[res_all_T_molten$measure == "cpu_time", ], aes(x = T, y = value, fill = method)) +
  geom_boxplot(show.legend = FALSE) +
  facet_wrap(~ method, dir = "v", scales = "free_y") +
  #coord_cartesian(ylim = c(0, 0.01)) +
  labs(title = "Elapsed CPU time for the estimation of all parameters", x = "T", y = "seconds")
```

<!---
The following line plots with confidence bands are more compact:
```{r, echo=FALSE, eval=FALSE, message=FALSE, fig.width = 9, fig.height = 5, out.width = "100%"}
library(dplyr)

# generate mean and confidence intervale in data.frame
res_meansd_T_molten <- 
  res_all_T_molten %>%
  mutate(T = as.numeric(as.character(T))) %>%
  group_by(method, measure, T) %>% 
  summarize(value_mean      = mean(value),
            value_meansd_lo = mean(value) - sd(value),
            value_meansd_up = mean(value) + sd(value))

# plot
ggplot(res_meansd_T_molten[res_meansd_T_molten$measure == "nu", ], aes(x = T, y = value_mean, color = method)) +
  geom_ribbon(aes(ymin = value_meansd_lo, ymax = value_meansd_up, fill = method), alpha = 0.3, linetype = "blank", show.legend = FALSE) +
  geom_line() + geom_point() +
  labs(title = sprintf("Estimation of nu (nu = %d, N = %d)", nu, N), y = "nu")
```
--->

Conclusions:

- The optimization of $\nu$ via the EM algorithm is not stable enough, so better not to use it.
- The three direct estimations "nu = kurtosis", "nu = MLE-diag", and "nu = MLE-diag-resampled" give good results. As a consequence, the faster method "nu = kurtosis" will be chosen by default.
- In terms of number of EM iterations, the fixed value or initial point for $\nu$ does not matter that much.
- If $\nu$ is optimized via the EM algorithm, then the initial point is not that relevant and one can simply initialize to $\nu_0 = 4$ with zero computational cost.




