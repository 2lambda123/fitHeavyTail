---
title: "Estimation of nu"
author: |
  | Daniel P. Palomar
  | The Hong Kong University of Science and Technology (HKUST)
date: "`r Sys.Date()`"
output:
  prettydoc::html_pretty:
    theme: tactile
    highlight: vignette
    toc: no
    toc_depth: 2
  html_document:
    theme: flatly
    highlight: pygments  
    toc: yes
    toc_depth: 2
params:
  N_realiz: 10
---

```{r, echo = FALSE}
library(knitr)
opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.align = "center",
  fig.retina = 2,
  out.width = "85%",
  dpi = 96,
  pngquant = "--speed=1"
)
knitr_in_progress <- isTRUE(getOption('knitr.in.progress'))
knit_hooks$set(pngquant = hook_pngquant)
# rmarkdown::render("vignettes/nu_estimation.Rmd", "html_document", params = list(N_realiz = 500))
```

-----------

> This report explores the convergence of $\nu$ for different estimation methods in package [`fitHeavyTail`](https://CRAN.R-project.org/package=fitHeavyTail).


Let's define the parameters for the generation of the multivariate Student's $t$ distribution:
```{r}
library(mvtnorm)  # package for multivariate t distribution
set.seed(137)

N <- 20   # number of variables
T <- 60   # number of observations
nu <- 4   # degrees of freedom for tail heavyness
mu <- rep(0, N)
U <- t(rmvnorm(n = round(0.3*N), sigma = 0.1*diag(N)))
Sigma_cov <- U %*% t(U) + diag(N)  # covariance matrix with factor model structure
Sigma_scatter <- (nu-2)/nu * Sigma_cov
```


To warm up, let's consider the following estimators for $\nu$:

- based on the kurtosis directly
- based on MLE w.r.t. $\nu$ assuming a diagonal sample covariance matrix
- idem but resampling for additional stability
- iterative estimation based on the EM algorithm with the previous three initial points. 

```{r}
library(fitHeavyTail)

X <- rmvt(n = T, delta = mu, sigma = Sigma_scatter, df = nu)  # generate data

# direct methods
nu_kurtosis           <- fit_mvt(X, nu = "kurtosis")$nu
nu_mle_diag           <- fit_mvt(X, nu = "MLE-diag")$nu
nu_mle_diag_resampled <- fit_mvt(X, nu = "MLE-diag-resampled")$nu

# EM with different initial points
EM_nu0_true               <- fit_mvt(X, nu = "iterative", initial = list(nu = nu))$nu
EM_nu0_kurtosis           <- fit_mvt(X, nu = "iterative", initial = list(nu = "kurtosis"))$nu
EM_nu0_mle_diag           <- fit_mvt(X, nu = "iterative", initial = list(nu = "MLE-diag"))$nu
EM_nu0_mle_diag_resampled <- fit_mvt(X, nu = "iterative", initial = list(nu = "MLE-diag-resampled"))$nu

rbind("nu_true" = nu, nu_kurtosis, nu_mle_diag, nu_mle_diag_resampled,
      EM_nu0_true, EM_nu0_kurtosis, EM_nu0_mle_diag, EM_nu0_mle_diag_resampled)
```

We can observe the convergence of $\nu$:
```{r}
fitted <- fit_mvt(X, nu = "iterative", initial = list(nu = nu), nu_iterative_method = "theta-0", return_iterates = TRUE)
fitHeavyTail:::plot_convergence(fitted)$nu
```


We now run `r params$N_realiz` Monte Carlo simulations.
```{r, echo=FALSE}
library(parallel)  # detectCores(logical = FALSE)
#options(nu_min = 4.2)

MSE_cov     <- function(Sigma_cov_hat) sum((Sigma_cov_hat - Sigma_cov)^2) / sum(Sigma_cov^2)
MSE_scatter <- function(Sigma_scatter_hat) sum((Sigma_scatter_hat - Sigma_scatter)^2) / sum(Sigma_scatter^2)
MSE_fnu     <- function(nu_hat) (nu_hat/(nu_hat-2) - nu/(nu-2))^2
factor_scatter_oracle <- function(Sigma_scatter_hat) 1/(sum((Sigma_scatter_hat - Sigma_scatter)^2) / sum(Sigma_scatter^2) + 1)
factor_cov_oracle     <- function(Sigma_cov_hat)     1/(sum((Sigma_cov_hat - Sigma_cov)^2) / sum(Sigma_cov^2) + 1)
factor_cov <- function(X, mu, nu) {
  T <- nrow(X)
  N <- ncol(X)
  Xc <- X - matrix(mu, T, N, byrow = TRUE)
  gamma <- covShrinkage:::gamma_S(covShrinkage:::sign_covmat(Xc), T)
  
  # for SCM 
  kappa <- 2/(nu+2 - 4)
  #kappa <- covShrinkage:::kappa_ell(Xc, method = "empirical marginals")
  #NMSE <- 1/gamma/T * (kappa*(2*gamma + N) + gamma + N)
  NMSE <- 1/T * (kappa*(2 + N) + 1 + N)  # setting gamma = 1
  1/(NMSE + 1)

  # # for heavy-tailed estimator   <--  this is wrong as it is always 1
  # psi1 <- (N + nu)/(2 + N + nu)
  # nu/(nu-2) * T*gamma/(gamma*psi1/N + psi1-1 + T*gamma*nu/(nu-2))
}


eval_single <- function(X) {
  factor_cov_hat <- factor_scatter_hat <- factor_cov <- factor_scatter <- 
    nu_hat <- MSE_fnu <- MSE_scatter <- MSE_cov <- num_iterations <- cpu_time <- list()
  T <- nrow(X)

  name                     <- "nu = kurtosis"
  fitted                   <- fit_mvt(X, nu = "kurtosis", scale_minMSE = FALSE)
  nu_hat[name]             <- fitted$nu
  MSE_fnu[name]            <- MSE_fnu(fitted$nu)
  factor_cov[name]         <- factor_cov_oracle(fitted$cov)
  factor_cov_hat[name]     <- factor_cov(X, fitted$mu, fitted$nu)
  factor_scatter[name]     <- factor_scatter_oracle(fitted$scatter)
  factor_scatter_hat[name] <- (fitted$nu-2)/fitted$nu * factor_cov_hat[[name]]
  MSE_cov[name]            <- MSE_cov(fitted$cov)
  MSE_scatter[name]        <- MSE_scatter(fitted$scatter)
  num_iterations[name]     <- fitted$num_iterations
  cpu_time[name]           <- fitted$cpu_time

  name                     <- "nu0 = kurtosis + ECME-diag"
  fitted                   <- fit_mvt(X, nu = "iterative", initial = list(nu = "kurtosis"), nu_iterative_method = "ECME-diag", scale_minMSE = TRUE)
  nu_hat[name]             <- fitted$nu
  MSE_fnu[name]            <- MSE_fnu(fitted$nu)
  factor_cov[name]         <- factor_cov_oracle(fitted$cov)
  factor_cov_hat[name]     <- factor_cov(X, fitted$mu, fitted$nu)
  factor_scatter[name]     <- factor_scatter_oracle(fitted$scatter)
  factor_scatter_hat[name] <- (fitted$nu-2)/fitted$nu * factor_cov_hat[[name]]
  MSE_cov[name]            <- MSE_cov(fitted$cov)
  MSE_scatter[name]        <- MSE_scatter(fitted$scatter)
  num_iterations[name]     <- fitted$num_iterations
  cpu_time[name]           <- fitted$cpu_time

    
  name                     <- "nu0 = kurtosis + theta-0"
  fitted                   <- fit_mvt(X, nu = "iterative", initial = list(nu = "kurtosis"), nu_iterative_method = "theta-0", scale_minMSE = TRUE)
  nu_hat[name]             <- fitted$nu
  MSE_fnu[name]            <- MSE_fnu(fitted$nu)
  factor_cov[name]         <- factor_cov_oracle(fitted$cov)
  factor_cov_hat[name]     <- factor_cov(X, fitted$mu, fitted$nu)
  factor_scatter[name]     <- factor_scatter_oracle(fitted$scatter)
  factor_scatter_hat[name] <- (fitted$nu-2)/fitted$nu * factor_cov_hat[[name]]
  MSE_cov[name]            <- MSE_cov(fitted$cov)
  MSE_scatter[name]        <- MSE_scatter(fitted$scatter)
  num_iterations[name]     <- fitted$num_iterations
  cpu_time[name]           <- fitted$cpu_time


  name                     <- "nu0 = kurtosis + theta-1b"
  fitted                   <- fit_mvt(X, nu = "iterative", initial = list(nu = "kurtosis"), nu_iterative_method = "theta-1b", scale_minMSE = TRUE)
  nu_hat[name]             <- fitted$nu
  MSE_fnu[name]            <- MSE_fnu(fitted$nu)
  factor_cov[name]         <- factor_cov_oracle(fitted$cov)
  factor_cov_hat[name]     <- factor_cov(X, fitted$mu, fitted$nu)
  factor_scatter[name]     <- factor_scatter_oracle(fitted$scatter)
  factor_scatter_hat[name] <- (fitted$nu-2)/fitted$nu * factor_cov_hat[[name]]
  MSE_cov[name]            <- MSE_cov(fitted$cov)
  MSE_scatter[name]        <- MSE_scatter(fitted$scatter)
  num_iterations[name]     <- fitted$num_iterations
  cpu_time[name]           <- fitted$cpu_time

  
  name                     <- "nu0 = kurtosis + theta-2a"
  fitted                   <- fit_mvt(X, nu = "iterative", initial = list(nu = "kurtosis"), nu_iterative_method = "theta-2a", scale_minMSE = TRUE)
  nu_hat[name]             <- fitted$nu
  MSE_fnu[name]            <- MSE_fnu(fitted$nu)
  factor_cov[name]         <- factor_cov_oracle(fitted$cov)
  factor_cov_hat[name]     <- factor_cov(X, fitted$mu, fitted$nu)
  factor_scatter[name]     <- factor_scatter_oracle(fitted$scatter)
  factor_scatter_hat[name] <- (fitted$nu-2)/fitted$nu * factor_cov_hat[[name]]
  MSE_cov[name]            <- MSE_cov(fitted$cov)
  MSE_scatter[name]        <- MSE_scatter(fitted$scatter)
  num_iterations[name]     <- fitted$num_iterations
  cpu_time[name]           <- fitted$cpu_time

  
  name                     <- "nu0 = kurtosis + theta-2b"
  fitted                   <- fit_mvt(X, nu = "iterative", initial = list(nu = "kurtosis"), nu_iterative_method = "theta-2b", scale_minMSE = TRUE)
  nu_hat[name]             <- fitted$nu
  MSE_fnu[name]            <- MSE_fnu(fitted$nu)
  factor_cov[name]         <- factor_cov_oracle(fitted$cov)
  factor_cov_hat[name]     <- factor_cov(X, fitted$mu, fitted$nu)
  factor_scatter[name]     <- factor_scatter_oracle(fitted$scatter)
  factor_scatter_hat[name] <- (fitted$nu-2)/fitted$nu * factor_cov_hat[[name]]
  MSE_cov[name]            <- MSE_cov(fitted$cov)
  MSE_scatter[name]        <- MSE_scatter(fitted$scatter)
  num_iterations[name]     <- fitted$num_iterations
  cpu_time[name]           <- fitted$cpu_time

  
  return(list("nu"          = nu_hat, 
              "MSE_fnu"     = MSE_fnu,
              "MSE_cov"     = MSE_cov, 
              "MSE_scatter" = MSE_scatter,  
              "factor_cov"         = factor_cov, 
              "factor_cov_hat"     = factor_cov_hat, 
              "factor_scatter"     = factor_scatter,  
              "factor_scatter_hat" = factor_scatter_hat,  
              "num_iterations" = num_iterations, 
              "cpu_time"       = cpu_time))
}


T_sweep <- round(seq(from = 30, to = 100, by = 10))
#T_sweep <- round(seq(from = 50, to = 200, by = 20))
if (!knitr_in_progress) pbar <- txtProgressBar(min = it <- 0, max = length(T_sweep), style = 3)
res_all_T <- list()
for(T in T_sweep) {
  if (!knitr_in_progress) setTxtProgressBar(pbar, it <- it + 1)
  
  # first, generate random heavy-tailed data sequentially for reproducibility
  X_list <- replicate(params$N_realiz, rmvt(n = T, delta = mu, sigma = Sigma_scatter, df = nu), simplify = FALSE)
  names(X_list) <- paste0("realiz ", 1:params$N_realiz)

  # then, run estimations for all realizations
  res_all_T <- c(res_all_T, list(lapply(X_list, eval_single)))
  #res_all_T <- c(res_all_T, list(mclapply(X_list, eval_single, mc.cores = 4)))
}
names(res_all_T) <- T_sweep
```

The results can be seen in the following boxplots:
```{r, echo=FALSE}
library(reshape2)

# create data.frame by melting the nested list
res_all_T_molten <- melt(res_all_T)
names(res_all_T_molten) <- c("value", "method", "measure", "realization", "T")
res_all_T_molten$realization <- NULL  # don't really need the realization index
res_all_T_molten$T      <- factor(res_all_T_molten$T, levels = T_sweep)
res_all_T_molten$method <- factor(res_all_T_molten$method, levels = names(res_all_T[[1]][[1]][[1]]))
```

```{r, echo=FALSE, fig.width=9, fig.height=5, out.width="100%"}
library(ggplot2)
ggplot(res_all_T_molten[res_all_T_molten$measure == "nu", ], aes(x = T, y = value, fill = method)) +
  geom_boxplot(show.legend = FALSE) +
  facet_wrap(~ method, dir = "v", scales = "free_y") +
  #coord_cartesian(ylim = c(0, 25)) +
  coord_cartesian(ylim = c(2, 8)) +
  geom_hline(yintercept = nu, linetype = "dashed") +
  labs(title = sprintf("Estimated nu (nu = %d, N = %d)", nu, N), x = "T", y = "nu")

# ggplot(res_all_T_molten[res_all_T_molten$measure == "MSE_fnu", ], aes(x = T, y = value, fill = method)) +
#   geom_boxplot(show.legend = FALSE) +
#   facet_wrap(~ method, dir = "v", scales = "free_y") +
#   coord_cartesian(ylim = c(0, 0.3)) +
#   labs(title = sprintf("Estimation error in nu/(nu-2) (nu = %d, N = %d)", nu, N), x = "T", y = "error")
```
<!---Note that except with the methods `nu = "MLE-diag-resampled"` and `"nu0 = kurtosis + trace-fitting"`, all the others have huge outliers outside the y-range shown (up to 100 which is the hard limit).
<br><br><br>--->

```{r, echo=FALSE, fig.width=9, fig.height=5, out.width="100%"}
# ggplot(res_all_T_molten[res_all_T_molten$measure == "factor_cov_hat", ], aes(x = T, y = value, fill = method)) +
#   geom_boxplot(show.legend = FALSE) +
#   facet_wrap(~ method, dir = "v", scales = "free_y") +
#   coord_cartesian(ylim = c(0, 1.5)) + #27 500
#   labs(title = sprintf("Estimation of factor that minimizes the MSE for the covariance matrix (nu = %d, N = %d)", nu, N), x = "T", y = "factor")
# 
# ggplot(res_all_T_molten[res_all_T_molten$measure == "factor_cov", ], aes(x = T, y = value, fill = method)) +
#   geom_boxplot(show.legend = FALSE) +
#   facet_wrap(~ method, dir = "v", scales = "free_y") +
#   coord_cartesian(ylim = c(0, 1.5)) + #27 500
#   labs(title = sprintf("Oracle factor that minimizes the MSE for the covariance matrix (nu = %d, N = %d)", nu, N), x = "T", y = "factor")

# ggplot(res_all_T_molten[res_all_T_molten$measure == "factor_scatter_hat", ], aes(x = T, y = value, fill = method)) +
#   geom_boxplot(show.legend = FALSE) +
#   facet_wrap(~ method, dir = "v", scales = "free_y") +
#   coord_cartesian(ylim = c(0, 1.5)) + #27 500
#   labs(title = sprintf("Estimation of factor that minimizes the MSE for the scatter matrix (nu = %d, N = %d)", nu, N), x = "T", y = "factor")
# 
# ggplot(res_all_T_molten[res_all_T_molten$measure == "factor_scatter", ], aes(x = T, y = value, fill = method)) +
#   geom_boxplot(show.legend = FALSE) +
#   facet_wrap(~ method, dir = "v", scales = "free_y") +
#   coord_cartesian(ylim = c(0, 1.5)) + #10 150
#   labs(title = sprintf("Oracle factor that minimizes the MSE for the scatter matrix (nu = %d, N = %d)", nu, N), x = "T", y = "factor")
```

```{r, echo=FALSE, fig.width=9, fig.height=5, out.width="100%"}
ggplot(res_all_T_molten[res_all_T_molten$measure == "MSE_cov", ], aes(x = T, y = value, fill = method)) +
  geom_boxplot(show.legend = FALSE) +
  facet_wrap(~ method, dir = "v", scales = "free_y") +
  coord_cartesian(ylim = c(0, 1)) +
  labs(title = sprintf("Relative estimation error in covariance matrix (nu = %d, N = %d)", nu, N), x = "T", y = "error")

# ggplot(res_all_T_molten[res_all_T_molten$measure == "MSE_scatter", ], aes(x = T, y = value, fill = method)) +
#   geom_boxplot(show.legend = FALSE) +
#   facet_wrap(~ method, dir = "v", scales = "free_y") +
#   coord_cartesian(ylim = c(0, 1.5)) + #10 150
#   labs(title = sprintf("Relative estimation error in scatter matrix (nu = %d, N = %d)", nu, N), x = "T", y = "error")
```
<!---Ironically, all the iterative methods have huge outliers outside the y-range shown (up to 200-300). So better not to iteratively update nu! The best method now is `nu = "kurtosis"`.
<br><br><br>--->


```{r, echo=FALSE, message=FALSE, fig.width=9, fig.height=5, out.width="100%"}
library(dplyr)

# generate mean and confidence intervale in data.frame
res_meansd_T_molten <- 
  res_all_T_molten %>%
  mutate(T = as.numeric(as.character(T))) %>%
  group_by(method, measure, T) %>% 
  summarize(value_mean      = mean(value),
            value_meansd_lo = mean(value) - sd(value),
            value_meansd_up = mean(value) + sd(value))

# plot
ggplot(res_meansd_T_molten[res_meansd_T_molten$measure == "MSE_cov", ], aes(x = T, y = value_mean, color = method)) +
  #geom_ribbon(aes(ymin = value_meansd_lo, ymax = value_meansd_up, fill = method), alpha = 0.3, linetype = "blank", show.legend = FALSE) +
  geom_line() + geom_point() +
  labs(title = sprintf("Relative estimation error in covariance matrix (nu = %d, N = %d)", nu, N), x = "T", y = "error")
```





```{r, echo=FALSE, fig.width=9, fig.height=5, out.width="100%"}
ggplot(res_all_T_molten[res_all_T_molten$measure == "num_iterations", ], aes(x = T, y = value, fill = method)) +
  geom_boxplot(show.legend = FALSE) +
  facet_wrap(~ method, dir = "v", scales = "free_y") +
  labs(title = "Iterations for the estimation of all parameters", x = "T", y = "iterations")
```
In terms of iterations they all look similar, although the methods that do not update nu are slightly better..
<br><br><br>

```{r, echo=FALSE, fig.width=9, fig.height=5, out.width="100%"}
ggplot(res_all_T_molten[res_all_T_molten$measure == "cpu_time", ], aes(x = T, y = value, fill = method)) +
  geom_boxplot(show.legend = FALSE) +
  facet_wrap(~ method, dir = "v", scales = "free_y") +
  coord_cartesian(ylim = c(0, 0.02)) +
  labs(title = "Elapsed CPU time for the estimation of all parameters", x = "T", y = "seconds")
```




Conclusions:

- The optimization of $\nu$ via the EM algorithm is not stable enough, so better not to use it.
- The three direct estimations "nu = kurtosis", "nu = MLE-diag", and "nu = MLE-diag-resampled" give good results. As a consequence, the faster method "nu = kurtosis" will be chosen by default.
- In terms of number of EM iterations, the fixed value or initial point for $\nu$ does not matter that much.
- If $\nu$ is optimized via the EM algorithm, then the initial point is not that relevant and one can simply initialize to $\nu_0 = 4$ with zero computational cost.




